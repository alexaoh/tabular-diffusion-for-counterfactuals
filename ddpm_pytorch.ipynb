{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2aba75",
   "metadata": {},
   "source": [
    "# DDPM with Pytorch for Adult.\n",
    "\n",
    "This is our first attempt at implementing a DDGM using Python and Pytorch. Our basis is the code from [here](https://github.com/jmtomczak/intro_dgm/blob/main/ddgms/ddgm_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "862a199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n",
      "The working directory is /home/ajo/gitRepos/master_thesis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics # plot_roc_curve.\n",
    "from sklearn.model_selection import train_test_split # Train/test/validation split of data.\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import random \n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "# Configure the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Print working directory (for control)\n",
    "import os\n",
    "print(f\"The working directory is {os.getcwd()}\")\n",
    "\n",
    "# Set seeds for reproducibility. \n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a19f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Adult data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "389a2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, p_dnns, decoder_net, beta, T, D):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.p_dnns = p_dnns # A list of sequentials: A single Sequential defines a DNN to \n",
    "                             # parameterize a distribution p(x_i|x_{i+1}) (reverse process).\n",
    "        self.decoder_net = decoder_net # The last MLP for p(x_0|x_1).\n",
    "        # I would prefer to define it here later. We can make this change after implementing the rest. \n",
    "        \n",
    "        self.D = D # Dimensionality of inputs (necessary for sampling).\n",
    "        self.T = T # Number of steps (or latent variables).\n",
    "        self.beta = torch.FloatTensor([beta]) # Betas for forward process (\"encoding\"). \n",
    "                                              # Is essentially the fixed variance for diffusion.\n",
    "        \n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        \"\"\"Reparameterization for Gaussian distribution.\"\"\"\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def reparameterization_gaussian_diffusion(self, x, i):\n",
    "        \"\"\"Reparameterization for Gaussian forward diffusion.\n",
    "        \n",
    "        This is reparameterization following the iterative forward equation:\n",
    "        q(x_t|x_{t-1}) = N(sqrt(1-beta) * x_{t-1}, beta * I)\n",
    "    \n",
    "        \"\"\"\n",
    "        # i-argumentet brukes ikke her derimot, det må jo mangle!\n",
    "        # Mulig indeksen skal inn i self.beta tenker jeg, for å kun få en skalar som retur-verdi!\n",
    "        # Sånn må det nok være for at forward diffusjonen i forward-funksjonen skal bli rett!\n",
    "        return torch.sqrt(1.0 - self.beta)*x + torch.sqrt(self.beta)*torch.randn_like(x)\n",
    "    \n",
    "    def forward(self, x, reduction = \"avg\"):\n",
    "        \"\"\"Forward process in the neural net.\"\"\"\n",
    "        \n",
    "        ######## Forward diffusion process.\n",
    "        # The code's original author notes that we \"just wander around in the space using Gaussian random walk\".\n",
    "        \n",
    "        # Save the latent variables x_1, \\ldots, x_T in a list. \n",
    "        latents = [self.reparameterization_gaussian_diffusion(x, 0)]\n",
    "        for i in range(1, self.T):\n",
    "            latents.append(self.reparameterization_gaussian_diffusion(latents[-1], i))\n",
    "        # This should obviously be extended with the closed formula for any time step\n",
    "        # using alpha_bar etc, since that will give a lot less of a computational burden.\n",
    "            \n",
    "        ######## Backward diffusion process.\n",
    "        mus = []\n",
    "        log_vars = []\n",
    "        \n",
    "        for i in range(len(self.p_dnns) -1 , -1, -1):\n",
    "            h = self.p_dnns[i](latents[i+1]) # predict p(x_i|x_{i+1}).\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim = 1) # Get mu and log_var from the prediction.\n",
    "                            # This model predicts the variances as well, which is fixed in DDPM paper.\n",
    "            mus.append(mu_i) # Save the mu_i.\n",
    "            log_vars.append(log_var_i) # Save the log_var_i.\n",
    "            \n",
    "        # The last step, i.e. p(x_0|x_1):\n",
    "        # We assume the last distribution is Normal(x | tanh(NN(x_1)), 1).\n",
    "        # This assumptions is apparent from the decoder net that will be defined later. \n",
    "        mu_x = self.decoder_net(latents[0])\n",
    "        \n",
    "        \n",
    "        ######## ELBO.\n",
    "        # Reconstruction error. Equal to -MSE(x,mu_x) + constant.\n",
    "        RE = log_standard_normal(x - mu_x).sum(-1)\n",
    "        \n",
    "        # KL divergence. We need to calculate this for all levels of latents. \n",
    "        KL = (log_normal_diag(latents[-1], torch.sqrt(1.0 - self.beta) \\\n",
    "                * latents[-1], torch.log(self.beta)) - log_standard_normal(latents[-1])).sum(-1)\n",
    "        \n",
    "        for i in range(len(mus)):\n",
    "            KL_i = (log_normal_diag(latents[i], torch.sqrt (1.0 - self.beta) \\\n",
    "                    * latents[i], torch.log(self.beta)) - log_normal_diag(latents[i], \\\n",
    "                                                mus[i], log_vars[i])).sum(-1)\n",
    "        \n",
    "        # Final ELBO.\n",
    "        if reduction == \"sum\":\n",
    "            loss = -(RE - KL).sum()\n",
    "        else:\n",
    "            loss = -(RE - KL).mean()\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size = 64):\n",
    "        \"\"\"Sample from the model. Follow backward diffusion model to the beginning.\"\"\"\n",
    "        z = torch.randn([batch_size, self.D])\n",
    "        for i in range(len(self.p_dnns)-1, -1, -1):\n",
    "            h = self.p_dnns[i](z)\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim = 1)\n",
    "            z = self.reparameterization(torch.tanh(mu_i), log_var_i)\n",
    "            \n",
    "        mu_x = self.decoder_net(z)\n",
    "        return mu_x\n",
    "    \n",
    "    \n",
    "    def sample_diffusion(self, x):\n",
    "        \"\"\"Sample from last latent after forward diffusion ('sanity check').\n",
    "        \n",
    "        This should resemble white noise, since we are returning x_T.\n",
    "        \"\"\"\n",
    "        latents = [self.reparameterization_gaussian_diffusion(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            latents.append(self.reparameterization_gaussian_diffusion(latents[-1], i))\n",
    "\n",
    "        return latents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa727190",
   "metadata": {},
   "source": [
    "## This implementation simply goes through the entire foward diffusion chain and backward chain (all latents) in each training iteration.\n",
    "\n",
    "This makes it extremely simplistic and useless in practice. Does not use any time embeddings because of this for example, since it trains a different MLP for each latent x_1, \\ldots, x_T. \n",
    "\n",
    "\n",
    "However, take some inspiration from the training loop the authors has built, I though it was very smart! He has incorporated some validation losses, as well as model saving on best and early stopping after some patience parameter number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7359a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f39fe27332ffeeeb6ffe75af7dffd3eb492ee0eb1ce27a6d282d0c6f68c778b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
