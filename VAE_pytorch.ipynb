{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cab74a8",
   "metadata": {},
   "source": [
    "# VAE with Pytorch for Adult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80409f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n",
      "The working directory is /home/ajo/gitRepos/master_thesis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics # plot_roc_curve.\n",
    "from sklearn.model_selection import train_test_split # Train/test/validation split of data.\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import patsy # Is not needed anylonger since sklearn did what I wanted.\n",
    "import random \n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader # Not sure what \"Dataset\" is for atm.\n",
    "\n",
    "# Configure the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Print working directory (for control)\n",
    "import os\n",
    "print(f\"The working directory is {os.getcwd()}\")\n",
    "\n",
    "# Set seeds for reproducibility. \n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d699de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load the adult data. \n",
    "adult_data = pd.read_csv(\"adult_data_no_NA.csv\", index_col = 0)\n",
    "print(adult_data.shape) # Looks good!\n",
    "\n",
    "categorical_features = [\"workclass\",\"marital_status\",\"occupation\",\"relationship\", \\\n",
    "                        \"race\",\"sex\",\"native_country\"]\n",
    "numerical_features = [\"age\",\"fnlwgt\",\"education_num\",\"capital_gain\",\"capital_loss\",\"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d98175fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Data.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile Data.py\n",
    "# Classes for data.\n",
    "\n",
    "class Data():\n",
    "    \"\"\"Class for pre-processing data. It automatically encodes, splits and scales the data. \n",
    "    \n",
    "    Contains methods for standardization, encoding and train/test/validation splitting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        Pandas df with loaded data. \n",
    "    cat_features : list of strings.\n",
    "        List of categorical features. \n",
    "    num_features : list of string. \n",
    "        List of numerical features. \n",
    "    valid : Boolean \n",
    "        True if validation data should be made, False if not. \n",
    "        \n",
    "    Methods \n",
    "    -------\n",
    "    get_training_data :\n",
    "        Returns a tuple with training data (X,y).\n",
    "    get_test_data :\n",
    "        Returns a tuple with test data (X,y).   \n",
    "    get_validation_data :\n",
    "        Returns a tuple with validation data (X,y) (if applicable).\n",
    "    train_test_valid_split : \n",
    "        Returns a tuple with (X_train, y_train, X_test, y_test) or \n",
    "        (X_train, y_train, X_test, y_test, X_valid, y_valid).\n",
    "    scale : \n",
    "        Scale the numerical features according to X_train.\n",
    "    descale : \n",
    "        Descale the numerical features according to X_train.\n",
    "    fit_scaler :\n",
    "        Fit sklearn scaler to X_train.\n",
    "    encode :\n",
    "        Encode the categorical features according to X_train.\n",
    "    decode :\n",
    "        Decode the categorical features according to X_train.\n",
    "    fit_encoder :\n",
    "        Fit sklearn encoder to X_train.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, data, cat_features, num_features, valid = False):\n",
    "        # The transformations are then done here. \n",
    "        self._data = data\n",
    "        self.categorical_features = cat_features\n",
    "        self.numerical_features = num_features\n",
    "        self.valid = valid\n",
    "        \n",
    "        # Assume output always is called 'y'.\n",
    "        self._X = data.loc[:, data.columns != \"y\"]\n",
    "        self._y = data.loc[:,\"y\"] \n",
    "        \n",
    "        # Encode the categorical features. \n",
    "        self.encoder = self.fit_encoder() # Fit the encoder to the categorical data.\n",
    "        self.X_encoded = self.encode()\n",
    "        \n",
    "        # Split into train/test/valid.\n",
    "        if self.valid:\n",
    "            (self.X_train, self.y_train, self.X_test, self.y_test, \\\n",
    "                self.X_valid, self.y_valid) = self.train_test_valid_split(self.X_encoded, self._y)\n",
    "        else:\n",
    "            (self.X_train, self.y_train, self.X_test, self.y_test) = self.train_test_valid_split(self.X_encoded, self._y)\n",
    "        \n",
    "        \n",
    "        # Scale the numerical features. \n",
    "        self.scaler = self.fit_scaler()\n",
    "        self.X_train = self.scale(self.X_train) # Scale the training data.\n",
    "        self.X_test = self.scale(self.X_test) # Scale the test data.\n",
    "        if self.valid:\n",
    "            self.X_valid = self.scale(self.X_valid) # Scale the validation data. \n",
    "        \n",
    "    \n",
    "    def get_training_data(self):\n",
    "        \"\"\"Returns training data (X_train, y_train).\"\"\"\n",
    "        return self.X_train, self.y_train\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        \"\"\"Returns test data (X_test, y_test).\"\"\"\n",
    "        return self.X_test, self.y_test\n",
    "    \n",
    "    def get_validation_data(self):\n",
    "        \"\"\"Returns validation data (X_valid, y_valid) if applicable.\"\"\"\n",
    "        if self.valid:\n",
    "            return self.X_valid, self.y_valid\n",
    "        else: \n",
    "            raise ValueError(\"You did not instantiate this object to contain validation data.\")\n",
    "    \n",
    "    def train_test_valid_split(self, X, y):\n",
    "        \"\"\"Split data into training/testing/validation, where validation is optional at instantiation.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42)\n",
    "        if self.valid:\n",
    "            X_test, X_valid, y_test, y_valid = train_test_split( \\\n",
    "                                        X_test, y_test, test_size=1/3, random_state=42)\n",
    "            return (X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "        return (X_train, y_train, X_test, y_test)\n",
    "            \n",
    "    def scale(self, df):\n",
    "        \"\"\"Scale the numerical features according to the TRAINING data.\"\"\"\n",
    "        output = df.copy() # Deep copy the given df. \n",
    "        output[self.numerical_features] = self.scaler.transform(output[self.numerical_features])\n",
    "        return output\n",
    "        \n",
    "    def descale(self, df):\n",
    "        \"\"\"Descale the numerical features according to the TRAINING data.\"\"\"\n",
    "        output = df.copy()\n",
    "        output[self.numerical_features] = self.scaler.inverse_transform(output[self.numerical_features])\n",
    "        return output\n",
    "\n",
    "    def fit_scaler(self):\n",
    "        \"\"\"Fit the scaler to the numerical TRAINING data. Only supports OneHotEncoding.\"\"\"\n",
    "        return preprocessing.StandardScaler().fit(self.X_train[self.numerical_features])\n",
    "    \n",
    "    def encode(self):\n",
    "        \"\"\"Encode the categorical data. Only supports OneHotEncoding.\"\"\"\n",
    "        output = self._X.copy() # Deep copy the X-data.\n",
    "        encoded_features = self.encoder.get_feature_names(self.categorical_features) # Get the encoded names. \n",
    "        \n",
    "        # Add the new columns to the new dataset (all the levels of the categorical features).\n",
    "        output[encoded_features] = self.encoder.transform(output[self.categorical_features])\n",
    "\n",
    "        # Remove the old columns (before one-hot encoding)\n",
    "        output = output.drop(self.categorical_features, axis = 1) \n",
    "        return output\n",
    "    \n",
    "    def decode(self, df):\n",
    "        \"\"\"Decode the categorical data. Only support OneHotEncoding.\"\"\"\n",
    "        output = df.copy()\n",
    "        encoded_features = self.encoder.get_feature_names(self.categorical_features) # Get the encoded names. \n",
    "        \n",
    "        if len(encoded_features) == 0:\n",
    "            return output # Does not work when there are not categorical features in df.\n",
    "        \n",
    "        output[self.categorical_features] = self.encoder.inverse_transform(output[encoded_features])\n",
    "        output = output.drop(encoded_features, axis=1)\n",
    "        return output\n",
    "    \n",
    "    def fit_encoder(self):\n",
    "        \"\"\"Fit the encoder to the categorical data. Only supports OneHotEncoding.\"\"\"\n",
    "        return preprocessing.OneHotEncoder(handle_unknown = \"error\", \\\n",
    "          sparse = False, drop = None).fit(self._X[self.categorical_features])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb64bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       workclass       marital_status       occupation relationship    race  \\\n",
      "count      30148                30148            30148        30148   30148   \n",
      "unique         7                    7               14            6       5   \n",
      "top      Private   Married-civ-spouse   Prof-specialty      Husband   White   \n",
      "freq       22151                14080             4050        12504   25931   \n",
      "\n",
      "          sex  native_country  \n",
      "count   30148           30148  \n",
      "unique      2              40  \n",
      "top      Male   United-States  \n",
      "freq    20386           27533  \n",
      "               age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
      "count  30148.00000  3.014800e+04   30148.000000  30148.000000  30148.000000   \n",
      "mean      38.54249  1.896485e+05      10.124453   1091.022788     87.993200   \n",
      "std       13.24241  1.059980e+05       2.565913   7519.182124    403.737188   \n",
      "min       17.00000  1.349200e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.00000  1.172680e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.00000  1.782450e+05      10.000000      0.000000      0.000000   \n",
      "75%       47.00000  2.378410e+05      13.000000      0.000000      0.000000   \n",
      "max       90.00000  1.490400e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours_per_week  \n",
      "count    30148.000000  \n",
      "mean        40.939697  \n",
      "std         12.015423  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "(30148, 13)\n"
     ]
    }
   ],
   "source": [
    "# Time to test the class out. \n",
    "Adult = Data(adult_data, categorical_features, numerical_features, valid = True)\n",
    "X_train, y_train = Adult.get_training_data()\n",
    "X_test, y_test = Adult.get_test_data()\n",
    "X_valid, y_valid = Adult.get_validation_data()\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(X_valid.shape)\n",
    "\n",
    "# Test descaling the already scaled data sets.\n",
    "X_train_descaled = Adult.descale(X_train)\n",
    "#print(X_train_descaled.shape)\n",
    "#print(X_train_descaled[numerical_features].describe())\n",
    "\n",
    "# Test decoding from one-hot encoding.\n",
    "X_train_decoded = Adult.decode(X_train)\n",
    "#print(X_train_decoded.shape)\n",
    "#print(X_train_decoded[categorical_features].describe())\n",
    "\n",
    "# Decoded and descaled data set should be the same as original (training) data. \n",
    "X_train_descaled = Adult.descale(X_train)\n",
    "X_train_de_everything = Adult.decode(X_train_descaled)\n",
    "print(X_train_de_everything[categorical_features].describe())\n",
    "print(X_train_de_everything[numerical_features].describe())\n",
    "print(X_train_de_everything.shape)\n",
    "# Looks like it all works as I intended!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcd07c",
   "metadata": {},
   "source": [
    "## The Dataset class for Pytorch below takes some data constructed from the Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65f11bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.4003, -0.2451, -0.0485, -0.1451, -0.2180, -1.9924,  0.0000,  0.0000,\n",
      "         1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000]), tensor([0.]))\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Class for using data with Pytorch.\"\"\"\n",
    "    def __init__(self, X, y, transform = None):\n",
    "        self.X = X.values.astype(np.float32) # Return a Numpy array with the dataframe contents. \n",
    "        self.y = np.reshape(y.values,(len(y.values),1)).astype(np.float32) # Return a Numpy array with the dataframe contents. \n",
    "\n",
    "        \n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.transform = transform\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.X[index], self.y[index]\n",
    "        #return sample\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ToTensor:\n",
    "    \"\"\"Callable object to transform CustomDataset inputs and labels to Pytorch tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        #print(labels)\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "# select rows from the dataset\n",
    "train_data = CustomDataset(X_train, y_train, transform = ToTensor()) # Here we put the Adult data loaded in earlier cells into the Dataset type. \n",
    "\n",
    "# Check if it works as expected.\n",
    "nex = train_data[0]\n",
    "print(nex)\n",
    "print(type(nex[0]), type(nex[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec5a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207f3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a777f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89888bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848602e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
