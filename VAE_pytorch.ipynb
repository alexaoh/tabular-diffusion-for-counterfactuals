{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cab74a8",
   "metadata": {},
   "source": [
    "# VAE with Pytorch for Adult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80409f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n",
      "The working directory is /home/ajo/gitRepos/master_thesis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics # plot_roc_curve.\n",
    "from sklearn.model_selection import train_test_split # Train/test/validation split of data.\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import random \n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "# Configure the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Print working directory (for control)\n",
    "import os\n",
    "print(f\"The working directory is {os.getcwd()}\")\n",
    "\n",
    "# Set seeds for reproducibility. \n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d699de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load the adult data. \n",
    "adult_data = pd.read_csv(\"adult_data_no_NA.csv\", index_col = 0)\n",
    "print(adult_data.shape) # Looks good!\n",
    "\n",
    "categorical_features = [\"workclass\",\"marital_status\",\"occupation\",\"relationship\", \\\n",
    "                        \"race\",\"sex\",\"native_country\"]\n",
    "numerical_features = [\"age\",\"fnlwgt\",\"education_num\",\"capital_gain\",\"capital_loss\",\"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile Data.py\n",
    "# Classes for data.\n",
    "\n",
    "class Data():\n",
    "    \"\"\"Class for pre-processing data. It automatically encodes, splits and scales the data. \n",
    "    \n",
    "    Contains methods for standardization, encoding and train/test/validation splitting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        Pandas df with loaded data. \n",
    "    cat_features : list of strings.\n",
    "        List of categorical features. \n",
    "    num_features : list of string. \n",
    "        List of numerical features. \n",
    "    valid : Boolean \n",
    "        True if validation data should be made, False if not. \n",
    "        \n",
    "    Methods \n",
    "    -------\n",
    "    get_training_data :\n",
    "        Returns a tuple with training data (X,y).\n",
    "    get_test_data :\n",
    "        Returns a tuple with test data (X,y).   \n",
    "    get_validation_data :\n",
    "        Returns a tuple with validation data (X,y) (if applicable).\n",
    "    train_test_valid_split : \n",
    "        Returns a tuple with (X_train, y_train, X_test, y_test) or \n",
    "        (X_train, y_train, X_test, y_test, X_valid, y_valid).\n",
    "    scale : \n",
    "        Scale the numerical features according to X_train.\n",
    "    descale : \n",
    "        Descale the numerical features according to X_train.\n",
    "    fit_scaler :\n",
    "        Fit sklearn scaler to X_train.\n",
    "    encode :\n",
    "        Encode the categorical features according to X_train.\n",
    "    decode :\n",
    "        Decode the categorical features according to X_train.\n",
    "    fit_encoder :\n",
    "        Fit sklearn encoder to X_train.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, data, cat_features, num_features, valid = False):\n",
    "        # The transformations are then done here. \n",
    "        self._data = data\n",
    "        self.categorical_features = cat_features\n",
    "        self.numerical_features = num_features\n",
    "        self.valid = valid\n",
    "        \n",
    "        # Assume output always is called 'y'.\n",
    "        self._X = data.loc[:, data.columns != \"y\"]\n",
    "        self._y = data.loc[:,\"y\"] \n",
    "        \n",
    "        # Encode the categorical features. \n",
    "        self.encoder = self.fit_encoder() # Fit the encoder to the categorical data.\n",
    "        self.X_encoded = self.encode()\n",
    "        \n",
    "        # Split into train/test/valid.\n",
    "        if self.valid:\n",
    "            (self.X_train, self.y_train, self.X_test, self.y_test, \\\n",
    "                self.X_valid, self.y_valid) = self.train_test_valid_split(self.X_encoded, self._y)\n",
    "        else:\n",
    "            (self.X_train, self.y_train, self.X_test, self.y_test) = self.train_test_valid_split(self.X_encoded, self._y)\n",
    "        \n",
    "        \n",
    "        # Scale the numerical features. \n",
    "        self.scaler = self.fit_scaler()\n",
    "        self.X_train = self.scale(self.X_train) # Scale the training data.\n",
    "        self.X_test = self.scale(self.X_test) # Scale the test data.\n",
    "        if self.valid:\n",
    "            self.X_valid = self.scale(self.X_valid) # Scale the validation data. \n",
    "        \n",
    "    \n",
    "    def get_training_data(self):\n",
    "        \"\"\"Returns training data (X_train, y_train).\"\"\"\n",
    "        return self.X_train, self.y_train\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        \"\"\"Returns test data (X_test, y_test).\"\"\"\n",
    "        return self.X_test, self.y_test\n",
    "    \n",
    "    def get_validation_data(self):\n",
    "        \"\"\"Returns validation data (X_valid, y_valid) if applicable.\"\"\"\n",
    "        if self.valid:\n",
    "            return self.X_valid, self.y_valid\n",
    "        else: \n",
    "            raise ValueError(\"You did not instantiate this object to contain validation data.\")\n",
    "    \n",
    "    def train_test_valid_split(self, X, y):\n",
    "        \"\"\"Split data into training/testing/validation, where validation is optional at instantiation.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42)\n",
    "        if self.valid:\n",
    "            X_test, X_valid, y_test, y_valid = train_test_split( \\\n",
    "                                        X_test, y_test, test_size=1/3, random_state=42)\n",
    "            return (X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "        return (X_train, y_train, X_test, y_test)\n",
    "            \n",
    "    def scale(self, df):\n",
    "        \"\"\"Scale the numerical features according to the TRAINING data.\"\"\"\n",
    "        output = df.copy() # Deep copy the given df. \n",
    "        output[self.numerical_features] = self.scaler.transform(output[self.numerical_features])\n",
    "        return output\n",
    "        \n",
    "    def descale(self, df):\n",
    "        \"\"\"Descale the numerical features according to the TRAINING data.\"\"\"\n",
    "        output = df.copy()\n",
    "        output[self.numerical_features] = self.scaler.inverse_transform(output[self.numerical_features])\n",
    "        return output\n",
    "\n",
    "    def fit_scaler(self):\n",
    "        \"\"\"Fit the scaler to the numerical TRAINING data. Only supports OneHotEncoding.\"\"\"\n",
    "        return preprocessing.StandardScaler().fit(self.X_train[self.numerical_features])\n",
    "    \n",
    "    def encode(self):\n",
    "        \"\"\"Encode the categorical data. Only supports OneHotEncoding.\"\"\"\n",
    "        output = self._X.copy() # Deep copy the X-data.\n",
    "        encoded_features = self.encoder.get_feature_names(self.categorical_features) # Get the encoded names. \n",
    "        \n",
    "        # Add the new columns to the new dataset (all the levels of the categorical features).\n",
    "        output[encoded_features] = self.encoder.transform(output[self.categorical_features])\n",
    "\n",
    "        # Remove the old columns (before one-hot encoding)\n",
    "        output = output.drop(self.categorical_features, axis = 1) \n",
    "        return output\n",
    "    \n",
    "    def decode(self, df):\n",
    "        \"\"\"Decode the categorical data. Only support OneHotEncoding.\"\"\"\n",
    "        output = df.copy()\n",
    "        encoded_features = self.encoder.get_feature_names(self.categorical_features) # Get the encoded names. \n",
    "        \n",
    "        if len(encoded_features) == 0:\n",
    "            return output # Does not work when there are not categorical features in df.\n",
    "        \n",
    "        output[self.categorical_features] = self.encoder.inverse_transform(output[encoded_features])\n",
    "        output = output.drop(encoded_features, axis=1)\n",
    "        return output\n",
    "    \n",
    "    def fit_encoder(self):\n",
    "        \"\"\"Fit the encoder to the categorical data. Only supports OneHotEncoding.\"\"\"\n",
    "        return preprocessing.OneHotEncoder(handle_unknown = \"error\", \\\n",
    "          sparse = False, drop = None).fit(self._X[self.categorical_features])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb64bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       workclass       marital_status       occupation relationship    race  \\\n",
      "count      30148                30148            30148        30148   30148   \n",
      "unique         7                    7               14            6       5   \n",
      "top      Private   Married-civ-spouse   Prof-specialty      Husband   White   \n",
      "freq       22151                14080             4050        12504   25931   \n",
      "\n",
      "          sex  native_country  \n",
      "count   30148           30148  \n",
      "unique      2              40  \n",
      "top      Male   United-States  \n",
      "freq    20386           27533  \n",
      "               age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
      "count  30148.00000  3.014800e+04   30148.000000  30148.000000  30148.000000   \n",
      "mean      38.54249  1.896485e+05      10.124453   1091.022788     87.993200   \n",
      "std       13.24241  1.059980e+05       2.565913   7519.182124    403.737188   \n",
      "min       17.00000  1.349200e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.00000  1.172680e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.00000  1.782450e+05      10.000000      0.000000      0.000000   \n",
      "75%       47.00000  2.378410e+05      13.000000      0.000000      0.000000   \n",
      "max       90.00000  1.490400e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours_per_week  \n",
      "count    30148.000000  \n",
      "mean        40.939697  \n",
      "std         12.015423  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "(30148, 13)\n"
     ]
    }
   ],
   "source": [
    "# Time to test the class out. \n",
    "Adult = Data(adult_data, categorical_features, numerical_features, valid = True)\n",
    "X_train, y_train = Adult.get_training_data()\n",
    "X_test, y_test = Adult.get_test_data()\n",
    "X_valid, y_valid = Adult.get_validation_data()\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(X_valid.shape)\n",
    "\n",
    "# Test descaling the already scaled data sets.\n",
    "X_train_descaled = Adult.descale(X_train)\n",
    "#print(X_train_descaled.shape)\n",
    "#print(X_train_descaled[numerical_features].describe())\n",
    "\n",
    "# Test decoding from one-hot encoding.\n",
    "X_train_decoded = Adult.decode(X_train)\n",
    "#print(X_train_decoded.shape)\n",
    "#print(X_train_decoded[categorical_features].describe())\n",
    "\n",
    "# Decoded and descaled data set should be the same as original (training) data. \n",
    "X_train_descaled = Adult.descale(X_train)\n",
    "X_train_de_everything = Adult.decode(X_train_descaled)\n",
    "print(X_train_de_everything[categorical_features].describe())\n",
    "print(X_train_de_everything[numerical_features].describe())\n",
    "print(X_train_de_everything.shape)\n",
    "# Looks like it all works as I intended!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcd07c",
   "metadata": {},
   "source": [
    "## The Dataset class for Pytorch below takes some data constructed from the Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f11bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.4003, -0.2451, -0.0485, -0.1451, -0.2180, -1.9924,  0.0000,  0.0000,\n",
      "         1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000]), tensor([0.]))\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Class for using data with Pytorch.\"\"\"\n",
    "    def __init__(self, X, y, transform = None):\n",
    "        self.X = X.values.astype(np.float32) # Return a Numpy array with the dataframe contents. \n",
    "        self.y = np.reshape(y.values,(len(y.values),1)).astype(np.float32) # Return a Numpy array with the dataframe contents. \n",
    "\n",
    "        \n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.transform = transform\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.X[index], self.y[index]\n",
    "        #return sample\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ToTensor:\n",
    "    \"\"\"Callable object to transform CustomDataset inputs and labels to Pytorch tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        #print(labels)\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "# select rows from the dataset\n",
    "train_data = CustomDataset(X_train, y_train, transform = ToTensor()) # Here we put the Adult data loaded in earlier cells into the Dataset type. \n",
    "\n",
    "# Check if it works as expected.\n",
    "nex = train_data[0]\n",
    "print(nex)\n",
    "print(type(nex[0]), type(nex[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f159441",
   "metadata": {},
   "source": [
    "## Time to implement the VAE\n",
    "\n",
    "We base our initial implementation on: https://github.com/jmtomczak/intro_dgm/blob/main/vaes/vae_example.ipynb\n",
    "It is somewhat modified by me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "62c39911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability distributions.\n",
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    \"\"\"Log-normal probability distribution.\"\"\"\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "    \n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    \"\"\"Log standard Gaussian distribution.\"\"\"\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4207f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Initialize the encoder net. Very simple with only one hidden layer with 15 units. \n",
    "        # Could make this arbitrarily more complex later. \n",
    "        self.encoder = nn.Sequential(nn.Linear(input_size, 15), nn.ReLU(),\n",
    "                                    nn.Linear(15, 2*latent_size))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        \"\"\"Reparameterization trick for Gaussian distributions.\"\"\"\n",
    "        std = torch.exp(0.5*log_var) # Get std from log_var.\n",
    "        eps = torch.randn_like(std) # Sample eps from N(0,1).\n",
    "        return mu + std * eps # Return sample from N(mu, std^2).\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes x, i.e. returns mu and log_var based on x.\"\"\"\n",
    "        h_e = self.encoder(x) # Encode x. Returns 2*hidden dim tensor. \n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim = 1) # Splits encoded tensor in two.\n",
    "        return mu_e, log_var_e\n",
    "    \n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        \"\"\"Samples from the encoder based on x. Returns a latent sample.\"\"\"\n",
    "        if (mu_e is None) and (log_var_e is None): # None supplied.\n",
    "            mu_e, log_var_e = self.encode(x) # Calculate mu and log_var if not provided. \n",
    "        else:\n",
    "            if (mu_e is None) or (log_var_e is None): # Both need to be supplied, not only one.\n",
    "                raise ValueError(\"'mu' and 'log_var' cannot be None.\")\n",
    "        z = self.reparameterization(mu_e, log_var_e)\n",
    "        return z\n",
    "    \n",
    "    def log_prob(self, x=None, mu_e=None,log_var_e=None, z=None):\n",
    "        \"\"\"Calculates log-probability.\"\"\"\n",
    "        if x is not None: # If x is supplied, we do not need the other arguments. \n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        else:\n",
    "            # If x is not supplied, mu_e, log_var and z need to be supplied.\n",
    "            if (mu_e is None) or (log_var_e is None) or (z is None):\n",
    "                raise ValueError(\"'mu', 'log_var' and 'z' cannot be None.\")\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "    \n",
    "    def forward(self, x, type=\"log_prob\"):\n",
    "        \"\"\"Forward pass for training. Returns either log-prob or sampling.\"\"\"\n",
    "        assert type in [\"encode\", \"log_prob\"], \"Type could be either 'encode' or 'log_prob'.\"\n",
    "        if type == \"log_prob\":\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d5a777f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, latent_size, distribution=\"gaussian\", num_vals = None):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.distribution = distribution # We only implement it for Gaussian (replicating our project).\n",
    "        self.num_vals = num_vals # This is needed for the categorical distribution.\n",
    "                                 # Thus it is not interesting for us now.\n",
    "        \n",
    "        # Initialize the decoder net. Very simple with only one hidden layer with 15 units. \n",
    "        # Could make this arbitrarily more complex later. \n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_size, 15), nn.ReLU(),\n",
    "                                    nn.Linear(15, output_size))\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode a sample z from the latent space. Returns mu, i.e. necessary parameter for p(x|z).\"\"\"\n",
    "        h_d = self.decoder(z)\n",
    "        if self.distribution == \"categorical\":\n",
    "            pass\n",
    "        elif self.distribution == \"bernoulli\":\n",
    "            pass\n",
    "        elif self.distribution == \"gaussian\":\n",
    "            # With the Gaussian we simply return the linear output.\n",
    "            return [h_d]\n",
    "        else: \n",
    "            raise ValueError(\"Only 'gaussian' decoder distribution is implemented.\")\n",
    "    \n",
    "    def sample(self, z):\n",
    "        \"\"\"Returns a reconstructed sample in the input space, based on latent sample z.\"\"\"\n",
    "        out = self.decode(z)\n",
    "        if self.distribution == \"categorical\":\n",
    "            pass\n",
    "        elif self.distribution == \"bernoulli\":\n",
    "            pass\n",
    "        elif self.distribution == \"gaussian\":\n",
    "            mu_d = out[0]\n",
    "            x_new = torch.normal(mu_d, torch.full_like(mu_d,np.sqrt(2))) # Try to replicate the same as in the project.\n",
    "        else:\n",
    "            raise ValueError(\"Only 'gaussian' decoder distribution is implemented.\")\n",
    "        return x_new\n",
    "    \n",
    "    def log_prob(self, x, z):\n",
    "        \"\"\"Calculated log-probability.\"\"\"\n",
    "        out = self.decode(z)\n",
    "        if self.distribution == \"categorical\":\n",
    "            pass\n",
    "        elif self.distribution == \"bernoulli\":\n",
    "            pass\n",
    "        elif self.distribution == \"gaussian\":\n",
    "            mu_d = out[0]\n",
    "            log_p = log_normal_diag(x, mu_d, torch.full_like(mu_d,np.log(2)),reduction = \"sum\", dim = -1)\n",
    "        else: \n",
    "            raise ValueError(\"Only 'gaussian' decoder distribution is implemented.\")\n",
    "        return log_p\n",
    "    \n",
    "    def forward(self, z, x, type = \"log_prob\"):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        assert type in ['decoder', 'log_prob'], \"Type could be either 'decode' or 'log_prob'\"\n",
    "        if type == \"log_prob\":\n",
    "            return self.log_prob(x, z)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d7ca7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    \"\"\"Prior for the latent variables. Standard Gaussian.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Prior, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        \"\"\"Samples from the standard Gaussian.\"\"\"\n",
    "        return torch.randn((batch_size, self.dim))\n",
    "        \n",
    "    def log_prob(self, z):\n",
    "        \"\"\"Calculates log-probability of the prior.\"\"\"\n",
    "        return log_standard_normal(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f89888bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, num_vals, likelihood_type = \"gaussian\"):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_size, latent_size).to(device)\n",
    "        self.decoder = Decoder(input_size, latent_size, \\\n",
    "                               distribution = likelihood_type, num_vals = num_vals).to(device)\n",
    "        self.prior = Prior(latent_size).to(device)\n",
    "        \n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "        \n",
    "    def forward(self, x, reduction = \"avg\"):\n",
    "        \"\"\"Forward pass for Pytorch.\"\"\"\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # Calculate ELBO.\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "                \n",
    "        if reduction == \"sum\":\n",
    "            return -(RE+KL).sum()\n",
    "        else:\n",
    "            return -(RE+KL).mean()\n",
    "\n",
    "    def sample(self, batch_size = 64):\n",
    "        \"\"\"Sample from the VAE using method 4 in my project (sample from latent prior and decode).\"\"\"\n",
    "        z = self.prior.sample(batch_size = batch_size).to(device)\n",
    "        return self.decoder.sample(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9848602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "batch_size = 16\n",
    "\n",
    "train_data = CustomDataset(X_train, y_train, transform = ToTensor()) \n",
    "test_data = CustomDataset(X_test, y_test, transform = ToTensor()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0c8fcdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=88, out_features=15, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=15, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=15, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=15, out_features=88, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (prior): Prior()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from pytorch_model_summary import summary\n",
    "# Hyperparameters. \n",
    "input_size = X_train.shape[1]\n",
    "latent_size = 8\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize a VAE model.\n",
    "vae_model = VAE(input_size, latent_size, 1).to(device)\n",
    "#print(summary(vae_model))\n",
    "print(vae_model)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c4c524e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [500/1885], Loss: 7148.9194\n",
      "Epoch [1/5], Step [1000/1885], Loss: 7149.0488\n",
      "Epoch [1/5], Step [1500/1885], Loss: 7149.3770\n",
      "Epoch [2/5], Step [500/1885], Loss: 7148.9453\n",
      "Epoch [2/5], Step [1000/1885], Loss: 7148.6523\n",
      "Epoch [2/5], Step [1500/1885], Loss: 7148.5630\n",
      "Epoch [3/5], Step [500/1885], Loss: 7148.7949\n",
      "Epoch [3/5], Step [1000/1885], Loss: 7149.3604\n",
      "Epoch [3/5], Step [1500/1885], Loss: 7151.2036\n",
      "Epoch [4/5], Step [500/1885], Loss: 7148.5469\n",
      "Epoch [4/5], Step [1000/1885], Loss: 7148.8140\n",
      "Epoch [4/5], Step [1500/1885], Loss: 7148.9951\n",
      "Epoch [5/5], Step [500/1885], Loss: 7148.8979\n",
      "Epoch [5/5], Step [1000/1885], Loss: 7148.5781\n",
      "Epoch [5/5], Step [1500/1885], Loss: 7148.5088\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "\n",
    "n_total_steps = len(train_loader) # Total length of training data. \n",
    "train_losses = []\n",
    "# Main training loop. \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader): \n",
    "        # Load the data on to the gpu.\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.view(labels.shape[0],1).to(device) \n",
    "        \n",
    "        # Forward pass.\n",
    "        #outputs = vae_model(inputs)\n",
    "        loss = vae_model.forward(inputs)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            train_losses.append(loss.item())\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a671e7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss per 'Iter'\")\n",
    "plt.xlabel(\"'Iter'\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "504c26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to try to generate some data from the VAE next. \n",
    "with torch.no_grad():\n",
    "    x_synth = vae_model.sample(30000) # Sample synthetic data. \n",
    "    x_synth = x_synth.cpu().numpy() # Change to cpu and convert to numpy 2D array. \n",
    "    df_synth = pd.DataFrame(x_synth, columns = X_test.columns) # Make dataframe of synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "288cfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()\n",
    "X_test_decoded = Adult.decode(X_test)\n",
    "X_test_decoded = Adult.descale(X_test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "37f6f343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10049.000000</td>\n",
       "      <td>1.004900e+04</td>\n",
       "      <td>10049.000000</td>\n",
       "      <td>10049.000000</td>\n",
       "      <td>10049.000000</td>\n",
       "      <td>10049.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.529704</td>\n",
       "      <td>1.892961e+05</td>\n",
       "      <td>10.106080</td>\n",
       "      <td>1121.994328</td>\n",
       "      <td>94.209474</td>\n",
       "      <td>40.930043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.176500</td>\n",
       "      <td>1.056462e+05</td>\n",
       "      <td>2.525525</td>\n",
       "      <td>7464.602533</td>\n",
       "      <td>417.381500</td>\n",
       "      <td>12.115749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.984700e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.173100e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.772650e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.366840e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.268339e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>3770.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
       "count  10049.000000  1.004900e+04   10049.000000  10049.000000  10049.000000   \n",
       "mean      38.529704  1.892961e+05      10.106080   1121.994328     94.209474   \n",
       "std       13.176500  1.056462e+05       2.525525   7464.602533    417.381500   \n",
       "min       17.000000  1.984700e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.173100e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.772650e+05      10.000000      0.000000      0.000000   \n",
       "75%       47.000000  2.366840e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.268339e+06      16.000000  99999.000000   3770.000000   \n",
       "\n",
       "       hours_per_week  \n",
       "count    10049.000000  \n",
       "mean        40.930043  \n",
       "std         12.115749  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_decoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "30c5dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data (real)\n",
      "Feature 'workclass' has 7 unique levels\n",
      "Feature 'marital_status' has 7 unique levels\n",
      "Feature 'occupation' has 14 unique levels\n",
      "Feature 'relationship' has 6 unique levels\n",
      "Feature 'race' has 5 unique levels\n",
      "Feature 'sex' has 2 unique levels\n",
      "Feature 'native_country' has 39 unique levels\n",
      "The sum of all levels is 80. This will be the number of cat-columns after one-hot encoding (non-full rank)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " Male      6735\n",
       " Female    3314\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test data (real)\")\n",
    "summer = 0\n",
    "for feat in categorical_features:\n",
    "    unq = len(X_test_decoded[feat].value_counts().keys().unique())\n",
    "    print(f\"Feature '{feat}' has {unq} unique levels\")\n",
    "    summer += unq\n",
    "print(f\"The sum of all levels is {summer}. This will be the number of cat-columns after one-hot encoding (non-full rank)\")\n",
    "# More interesting would be to print the number in each level!\n",
    "# But we might as well check if all the levels exist in the synthetic data anyway.\n",
    "\n",
    "X_test_decoded[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dc9f6026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.823616</td>\n",
       "      <td>177226.250000</td>\n",
       "      <td>9.350696</td>\n",
       "      <td>2473.951416</td>\n",
       "      <td>41.284409</td>\n",
       "      <td>42.191303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.834835</td>\n",
       "      <td>150404.062500</td>\n",
       "      <td>3.698177</td>\n",
       "      <td>10645.609375</td>\n",
       "      <td>580.359558</td>\n",
       "      <td>17.167130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-46.810154</td>\n",
       "      <td>-469077.531250</td>\n",
       "      <td>-5.810428</td>\n",
       "      <td>-39325.296875</td>\n",
       "      <td>-2407.562988</td>\n",
       "      <td>-26.675568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.048370</td>\n",
       "      <td>75238.914062</td>\n",
       "      <td>6.842318</td>\n",
       "      <td>-4759.497681</td>\n",
       "      <td>-347.510872</td>\n",
       "      <td>30.655611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.819935</td>\n",
       "      <td>176885.273438</td>\n",
       "      <td>9.381191</td>\n",
       "      <td>2466.070801</td>\n",
       "      <td>38.873844</td>\n",
       "      <td>42.201799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.485908</td>\n",
       "      <td>278834.031250</td>\n",
       "      <td>11.844203</td>\n",
       "      <td>9645.335693</td>\n",
       "      <td>433.424240</td>\n",
       "      <td>53.739108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.099205</td>\n",
       "      <td>795608.875000</td>\n",
       "      <td>24.402597</td>\n",
       "      <td>42133.832031</td>\n",
       "      <td>2384.441650</td>\n",
       "      <td>120.116699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age         fnlwgt  education_num  capital_gain  capital_loss  \\\n",
       "count  30000.000000   30000.000000   30000.000000  30000.000000  30000.000000   \n",
       "mean      38.823616  177226.250000       9.350696   2473.951416     41.284409   \n",
       "std       18.834835  150404.062500       3.698177  10645.609375    580.359558   \n",
       "min      -46.810154 -469077.531250      -5.810428 -39325.296875  -2407.562988   \n",
       "25%       26.048370   75238.914062       6.842318  -4759.497681   -347.510872   \n",
       "50%       38.819935  176885.273438       9.381191   2466.070801     38.873844   \n",
       "75%       51.485908  278834.031250      11.844203   9645.335693    433.424240   \n",
       "max      120.099205  795608.875000      24.402597  42133.832031   2384.441650   \n",
       "\n",
       "       hours_per_week  \n",
       "count    30000.000000  \n",
       "mean        42.191303  \n",
       "std         17.167130  \n",
       "min        -26.675568  \n",
       "25%         30.655611  \n",
       "50%         42.201799  \n",
       "75%         53.739108  \n",
       "max        120.116699  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synth.describe()\n",
    "df_synth_decoded = Adult.decode(df_synth)\n",
    "df_synth_decoded = Adult.descale(df_synth_decoded)\n",
    "df_synth_decoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1e76a50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data\n",
      "Feature 'workclass' has 7 unique levels\n",
      "Feature 'marital_status' has 7 unique levels\n",
      "Feature 'occupation' has 14 unique levels\n",
      "Feature 'relationship' has 6 unique levels\n",
      "Feature 'race' has 5 unique levels\n",
      "Feature 'sex' has 2 unique levels\n",
      "Feature 'native_country' has 41 unique levels\n",
      "The sum of all levels is 82. This will be the number of cat-columns after one-hot encoding (non-full rank)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " Female    15569\n",
       " Male      14431\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Synthetic data\")\n",
    "summer = 0\n",
    "for feat in categorical_features:\n",
    "    unq = len(df_synth_decoded[feat].value_counts().keys().unique())\n",
    "    print(f\"Feature '{feat}' has {unq} unique levels\")\n",
    "    summer += unq\n",
    "print(f\"The sum of all levels is {summer}. This will be the number of cat-columns after one-hot encoding (non-full rank)\")\n",
    "# More interesting would be to print the number in each level!\n",
    "# But we might as well check if all the levels exist in the synthetic data anyway.\n",
    "\n",
    "df_synth_decoded[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceaa77d",
   "metadata": {},
   "source": [
    "## Plots for comparison.\n",
    "\n",
    "It looks like the VAE sucks for now (not surprising as the loss function does not really decrease during training!). \n",
    "\n",
    "We make some plots that can be used to compare the synthetic and the original data sets (as done in R earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa900dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675a805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9a9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddbec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccacae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
